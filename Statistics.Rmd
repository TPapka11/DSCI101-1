---
title: "Statistical Foundations"
author: "Gregory J. Matthews"
date: "2023-04-11"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Statistical Foundations

 - Population vs Sample
 - Sample Statistics: What is a "statistic"?
 - Sampling Distribution
 - Bootstrapping
 - Outliers
 - Statistical Models
 - Confounding and accounting for other factors
 - p-values
 
### Population vs Sample 
 - Population is EVERYTHING. 
 - Sample is a subset of the population!

### Sample Statstics
  - What is a "statistic"?

### Sampling Distribution
  - How accurate is my statistic?  
  - Let's create a population and 
  
  
```{r}
set.seed(1234)
N <- 1000000
pop <- rnorm(N, 0, 1)

n <- 100
d <- sample(pop,n, replace = FALSE)
xbar <- mean(d)
xbar

getxbar <- function(i){
n <- 100
d <- sample(pop,n, replace = FALSE)
xbar <- mean(d)
return(xbar)
}

library(tidyverse)
nsim <- 10000
results <- map(1:nsim, getxbar)
hist(unlist(results))
unlist(results)
#sd = 0.1009027

getxbar <- function(i){
n <- 1000
d <- sample(pop,n, replace = FALSE)
xbar <- mean(d)
return(xbar)
}

library(tidyverse)
nsim <- 10000
results <- map(1:nsim, getxbar)
hist(unlist(results))
#sd = 0.03159831

getmedian <- function(i){
n <- 1000
d <- sample(pop,n, replace = FALSE)
xbar <- median(d)
return(xbar)
}

library(tidyverse)
nsim <- 10000
results <- map(1:nsim, getmedian)
hist(unlist(results))
sd(unlist(results))
#sd =  0.04008901


```
 
### The Bootstrap
 - Estimating standard errors.
 
 
```{r}
set.seed(1234)
N <- 1000000
pop <- rnorm(N, 0, 1)

n <- 100
d <- sample(pop,n, replace = FALSE)
xbar <- mean(d)
xbar

bootxbar <- function(i){
n <- length(d)
dboot <- sample(d,n, replace = TRUE)
xbar <- mean(dboot)
return(xbar)
}

nsim <- 10000
results <- map_dbl(1:nsim, bootxbar)
hist(results)
sd(results)




```

### Outliers
"Outliers can often tell us interesting things. How they should be handled depends on their cause. Outliers due to data irregularities or errors should be fixed. Other outliers may yield important insights. Outliers should never be dropped unless there is a clear rationale. If outliers are dropped this should be clearly reported." - Baumer and Horton

### Statistical Models
 - Structures that allow us to relate variables to one another
 - Famous Quote: "All models are wrong, but some are useful".  

### Confounding and accounting for other factors
 - Correlation does not imply causation! But WHY!?!?!?!
 
```{r}
library(mdsr)
library(tidyverse)
 SAT_2010 <- SAT_2010 %>%
  mutate(Salary = salary/1000)
SAT_plot <- ggplot(data = SAT_2010, aes(x = Salary, y = total)) + 
  geom_point() + 
  geom_smooth(method = "lm") + 
  ylab("Average total score on the SAT") + 
  xlab("Average teacher salary (thousands of USD)")
SAT_plot
```

```{r}
SAT_mod1 <- lm(total ~ Salary, data = SAT_2010)
broom::tidy(SAT_mod1)
```
```{r}
SAT_2010 %>%
  skim(sat_pct)
```


```{r}
SAT_2010 <- SAT_2010 %>%
  mutate(SAT_grp = ifelse(sat_pct <= 27, "Low", "High"))
SAT_2010 %>%
  group_by(SAT_grp) %>%
  count()
```

```{r}
SAT_plot %+% SAT_2010 + 
  aes(color = SAT_grp) + 
  scale_color_brewer("% taking\nthe SAT", palette = "Set2")
```

Famous Example of this: "Sex Bias in Graduate Admissions: Data from Berkley". Bickel, Hammell, and O'Connel, Science, Vol. 187, No. 4175, pp. 398-404.


### The perils of the p-value
"Always report the actual p-value (or a statement that it is less than some small value such as p < 0.0001) rather than just the decision (reject null vs. fail to reject the null). In addition, confidence intervals are often more interpretable and should be reported as well." - Baumer and Horton

Important paper: Wasserstein and Lazar, 2016

To help clarify these issues, the American Statistical Association endorsed a statement on p-values (Wasserstein and Lazar 2016) that laid out six useful principles:

- p-values can indicate how incompatible the data are with a specified statistical model.
- p-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone.
- Scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.
- Proper inference requires full reporting and transparency.
- A p-value, or statistical significance, does not measure the size of an effect or the importance of a result.
- By itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis.


### Predictive modeling 
- Inference vs prediction
- Cross Validation
- Bias vs Variance trade off
- An example.  Trying to predict "high incomes" (>$50K)
```{r}
library(tidyverse)
library(mdsr)
url <-
"http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"
census <- read_csv(
  url,
  col_names = c(
    "age", "workclass", "fnlwgt", "education", 
    "education_1", "marital_status", "occupation", "relationship", 
    "race", "sex", "capital_gain", "capital_loss", "hours_per_week", 
    "native_country", "income"
  )
) %>%
  mutate(income = factor(income))
glimpse(census)

library(tidymodels)
set.seed(364)
n <- nrow(census)
census_parts <- census %>%
  initial_split(prop = 0.8)

train <- census_parts %>%
  training()

test <- census_parts %>%
  testing()

list(train, test) %>%
  map_int(nrow)


pi_bar <- train %>%
  count(income) %>%
  mutate(pct = n / sum(n)) %>%
  filter(income == ">50K") %>%
  pull(pct)
#fraction of high earners.
pi_bar

mod_null <- logistic_reg(mode = "classification") %>%
  set_engine("glm") %>%
  fit(income ~ 1, data = train)


library(yardstick)
pred <- train %>%
  select(income, capital_gain) %>%
  bind_cols(
    predict(mod_null, new_data = train, type = "class")
  ) %>%
  rename(income_null = .pred_class)
accuracy(pred, income, income_null)

confusion_null <- pred %>%
  conf_mat(truth = income, estimate = income_null)
confusion_null

mod_log_1 <- logistic_reg(mode = "classification") %>%
  set_engine("glm") %>%
  fit(income ~ capital_gain, data = train)


train_plus <- train %>%
  mutate(high_earner = as.integer(income == ">50K"))

ggplot(train_plus, aes(x = capital_gain, y = high_earner)) + 
  geom_count(
    position = position_jitter(width = 0, height = 0.05), 
    alpha = 0.5
  ) + 
  geom_smooth(
    method = "glm", method.args = list(family = "binomial"), 
    color = "dodgerblue", lty = 2, se = FALSE
  ) + 
  geom_hline(aes(yintercept = 0.5), linetype = 3) + 
  scale_x_log10(labels = scales::dollar)

pred <- pred %>%
  bind_cols(
    predict(mod_log_1, new_data = train, type = "class")
  ) %>%
  rename(income_log_1 = .pred_class)

confusion_log_1 <- pred %>%
  conf_mat(truth = income, estimate = income_log_1)

confusion_log_1

accuracy(pred, income, income_log_1)

broom::tidy(mod_log_1)

exp(- 1.37 + 0.000335*10000)/(1+exp(- 1.37 + 0.000335*10000))

```
